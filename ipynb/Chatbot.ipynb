{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20199a0a-7204-40d5-9681-7f184faa72c2",
   "metadata": {},
   "source": [
    "# Gradio Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d747220-e5ba-448b-a0b8-fc810ad23523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\unsto\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries and Dependencies\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b52948-fff1-4744-bf4b-7c2e4388cfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\unsto\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model('best_model_val_loss.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7771d09-591d-41b1-920c-47524bc3e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format data for LSTM\n",
    "def create_dataset(dataset, look_back=3):\n",
    "    X = []\n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "        a = dataset[i:(i + look_back), :]\n",
    "        X.append(a)\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4857adf1-54db-42dd-8bbb-37f1d5c3c6e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# from sklearn.preprocessing import LabelEncoder\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# # Creating an instance of label encoder\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Now scale the data\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m scaled_data \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(filtered_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDegree\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLength\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThickness\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThickness_Difference\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal Thickness\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scaler' is not defined"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Creating an instance of label encoder\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# # Assuming 'Criteria' is a categorical column that needs to be converted\n",
    "# filtered_data['Criteria'] = label_encoder.fit_transform(filtered_data['Criteria'])\n",
    "\n",
    "# Now scale the data\n",
    "scaled_data = scaler.fit_transform(filtered_data[['Degree', 'Length', 'Thickness', 'Thickness_Difference', 'Original Thickness', 'Year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb640138-13e6-4968-9a42-4606945499c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the data\n",
    "def process_data(data):\n",
    "    try:\n",
    "        data['Year'] = pd.to_datetime(data['Date']).dt.year\n",
    "        filtered_data = data[data['Year'].isin([2020, 2021, 2022, 2023, 2024, 2025])]\n",
    "        if filtered_data.empty:\n",
    "            raise ValueError(\"No data for specified years.\")\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = scaler.fit_transform(filtered_data[['Degree', 'Length', 'Thickness', 'Original Thickness', 'Criteria']])\n",
    "\n",
    "        # Debugging: Check shapes\n",
    "        print(\"Scaled data shape:\", scaled_data.shape)\n",
    "\n",
    "        X = create_dataset(scaled_data, look_back=3)\n",
    "        if X.size == 0:\n",
    "            raise ValueError(\"Insufficient data for predictions.\")\n",
    "\n",
    "        # Debugging: Check shapes\n",
    "        print(\"Processed data shape:\", X.shape)\n",
    "\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 3))\n",
    "        return X\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error in data processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c535a-2e28-4757-ab4b-19460bd76d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the model's probability outputs as class labels\n",
    "def interpret_predictions(predictions, threshold_pass=0.75, threshold_warn=0.5):\n",
    "    labels = []\n",
    "    for pred in predictions.flatten():\n",
    "        if pred >= threshold_pass:\n",
    "            labels.append('Pass')\n",
    "        elif pred >= threshold_warn:\n",
    "            labels.append('Warn')\n",
    "        else:\n",
    "            labels.append('Fail')\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279fb15a-6ada-466e-9fb7-60dee2987158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function for Gradio Interface\n",
    "def predict_from_csv(file):\n",
    "    try:\n",
    "        data = pd.read_csv(file.name)\n",
    "        processed_data = process_data(data)\n",
    "        predictions = model.predict(processed_data)\n",
    "        prediction_classes = interpret_predictions(predictions)\n",
    "\n",
    "        # Ensure predictions are aligned with data length\n",
    "        data['Prediction'] = prediction_classes[:len(data)]\n",
    "        return data[['Year', 'Prediction']].to_string(index=False)\n",
    "    except Exception as e:\n",
    "        return f\"Error in prediction: {e}\"\n",
    "        print(\"Processed data shape for prediction:\", processed_data.shape)  # Add this line to debug\n",
    "        predictions = model.predict(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df867b4f-90a1-4d10-9d26-70e139a6a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio interface\n",
    "iface = gr.Interface(fn=predict_from_csv, inputs=\"file\", outputs=\"text\")\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca54a57-7af6-4ce3-9b93-0b287f8bf1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
